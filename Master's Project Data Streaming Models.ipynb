{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3XepovyFmzHj8ZWYaHx8D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Setup"],"metadata":{"id":"X3UL0sNtb40W"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import string\n","import seaborn as sns\n","!pip install -U scikit-multiflow\n","#resource for profanity: https://pypi.org/project/profanity-check/\n","!pip install alt-profanity-check\n","!pip install sklearn --upgrade\n","#resource for sentiment: https://realpython.com/python-nltk-sentiment-analysis/#using-nltks-pre-trained-sentiment-analyzer\n","!pip install nltk\n","from profanity_check import predict\n","import nltk\n","nltk.download([\n","\"names\",\n","\"stopwords\",\n","\"state_union\",\n","\"twitter_samples\",\n","\"movie_reviews\",\n","\"averaged_perceptron_tagger\",\n","\"vader_lexicon\",\n","\"punkt\"])\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","%matplotlib notebook\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install tensorflow-io\n","!pip install kafka-python\n","import os\n","from datetime import datetime\n","import time\n","import threading\n","import json\n","from kafka import KafkaProducer\n","from kafka.errors import KafkaError\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_io as tfio\n","!curl -sSOL https://downloads.apache.org/kafka/3.6.1/kafka_2.13-3.6.1.tgz\n","!tar -xzf kafka_2.13-3.6.1.tgz\n","!./kafka_2.13-3.6.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.6.1/config/zookeeper.properties\n","!./kafka_2.13-3.6.1/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.6.1/config/server.properties\n","!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n","!sleep 10\n","!./kafka_2.13-3.6.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic ah-tweets-train\n","!./kafka_2.13-3.6.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic ah-tweets-test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glHI1DxWb6IB","executionInfo":{"status":"ok","timestamp":1703448712497,"user_tz":480,"elapsed":172297,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"11e65bed-91e9-4421-a8d8-cebf9d95db0c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-multiflow\n","  Downloading scikit-multiflow-0.5.3.tar.gz (450 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/450.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (2.4.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.23.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.11.4)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (3.7.1)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.2.2)\n","Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2023.3.post1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->scikit-multiflow) (1.16.0)\n","Building wheels for collected packages: scikit-multiflow\n","  Building wheel for scikit-multiflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-multiflow: filename=scikit_multiflow-0.5.3-cp310-cp310-linux_x86_64.whl size=1254678 sha256=3fc32720c6d9c99d0bbe486be3e2b57c5500bdca302075ca71685a286ee501ec\n","  Stored in directory: /root/.cache/pip/wheels/6e/1b/56/45b17a6cf203d98000a45976cb0dd0c4c3f11960e6a505f231\n","Successfully built scikit-multiflow\n","Installing collected packages: scikit-multiflow\n","Successfully installed scikit-multiflow-0.5.3\n","Collecting alt-profanity-check\n","  Downloading alt-profanity-check-1.3.2.tar.gz (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting scikit-learn==1.3.2 (from alt-profanity-check)\n","  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from alt-profanity-check) (1.3.2)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->alt-profanity-check) (1.23.5)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->alt-profanity-check) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.2->alt-profanity-check) (3.2.0)\n","Building wheels for collected packages: alt-profanity-check\n","  Building wheel for alt-profanity-check (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for alt-profanity-check: filename=alt_profanity_check-1.3.2-py3-none-any.whl size=1866996 sha256=d4f1f5d9eb2039ed7092bc22cd5b3018e9186f6f403c593c27a8c42c8642035b\n","  Stored in directory: /root/.cache/pip/wheels/b3/b9/e1/e1ace2573792813935cd59a2f8a0cecc807bd2dbf69327a0b3\n","Successfully built alt-profanity-check\n","Installing collected packages: scikit-learn, alt-profanity-check\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed alt-profanity-check-1.3.2 scikit-learn-1.3.2\n","Collecting sklearn\n","  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package state_union to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/state_union.zip.\n","[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting tensorflow-io\n","  Downloading tensorflow_io-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.35.0 (from tensorflow-io)\n","  Downloading tensorflow_io_gcs_filesystem-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n","  Attempting uninstall: tensorflow-io-gcs-filesystem\n","    Found existing installation: tensorflow-io-gcs-filesystem 0.34.0\n","    Uninstalling tensorflow-io-gcs-filesystem-0.34.0:\n","      Successfully uninstalled tensorflow-io-gcs-filesystem-0.34.0\n","Successfully installed tensorflow-io-0.35.0 tensorflow-io-gcs-filesystem-0.35.0\n","Collecting kafka-python\n","  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kafka-python\n","Successfully installed kafka-python-2.0.2\n","Waiting for 10 secs until kafka and zookeeper services are up and running\n","Created topic ah-tweets-train.\n","Created topic ah-tweets-test.\n"]}]},{"cell_type":"code","source":["def error_callback(exc):\n","    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n","\n","def write_to_kafka(topic_name, items):\n","  count=0\n","  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n","  for message, key in items:\n","    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n","    count+=1\n","  producer.flush()\n","  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n","\n","def decode_kafka_item(item):\n","  message = tf.io.decode_csv(item.message, [[0.0] for i in range(1)])\n","  key = tf.strings.to_number(item.key)\n","  return (message, key)\n","\n","def decode_kafka_test_item(raw_message, raw_key):\n","  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(1)])\n","  key = tf.strings.to_number(raw_key)\n","  return (message, key)\n","\n","def decode_kafka_online_item(raw_message, raw_key):\n","  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(1)])\n","  key = tf.strings.to_number(raw_key)\n","  return (message, key)\n","\n","from sklearn import preprocessing\n","\n","# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n","def encode_text_dummy(df, name):\n","    dummies = pd.get_dummies(df[name])\n","    for x in dummies.columns:\n","        dummy_name = \"{}-{}\".format(name, x)\n","        df[dummy_name] = dummies[x]\n","    df.drop(name, axis=1, inplace=True)\n","\n","\n","# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n","def encode_text_index(df, name):\n","    le = preprocessing.LabelEncoder()\n","    df[name] = le.fit_transform(df[name])\n","    return le.classes_\n","\n","\n","# Encode a numeric column as zscores\n","def encode_numeric_zscore(df, name, mean=None, sd=None):\n","    if mean is None:\n","        mean = df[name].mean()\n","\n","    if sd is None:\n","        sd = df[name].std()\n","\n","    df[name] = (df[name] - mean) / sd\n","\n","from collections.abc import Sequence\n","# Split into train/test\n","from sklearn.model_selection import train_test_split\n","# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n","def to_xy(df, target):\n","    result = []\n","    for x in df.columns:\n","        if x != target:\n","            result.append(x)\n","    # find out the type of the target column.\n","    target_type = df[target].dtypes\n","    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n","    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n","    if target_type in (np.int64, np.int32):\n","        # Classification\n","        dummies = pd.get_dummies(df[target])\n","        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n","    else:\n","        # Regression\n","        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)"],"metadata":{"id":"mlPdd0tksKFW","executionInfo":{"status":"ok","timestamp":1703448712497,"user_tz":480,"elapsed":9,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Tweet Feature Extraction And Data Cleaning/Processing"],"metadata":{"id":"73xCRZezcOAx"}},{"cell_type":"code","source":["ah_tweets = pd.read_csv(\"/content/drive/MyDrive/Master's Project/hatespeech_text_label_vote_RESTRICTED_100K_v2.csv\")\n","#had to fix dataframe column names\n","ah_tweets = ah_tweets.rename(columns={\"Tweet text  Label   Votes for the majority label\": \"Tweet text\", \"Unnamed: 1\": \"Label\", \"Unnamed: 2\": \"Votes for the majority label\"})\n","#last column not needed\n","ah_tweets.drop(ah_tweets.columns[[2]], axis=1, inplace=True)\n","#\"spam\" labels not needed\n","ah_tweets = ah_tweets[ah_tweets['Label'] != \"spam\"]"],"metadata":{"id":"4EI1hjoeEvbF","executionInfo":{"status":"ok","timestamp":1703448714146,"user_tz":480,"elapsed":1656,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["ah_tweets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"DtzeDur3FEBC","executionInfo":{"status":"ok","timestamp":1703448714682,"user_tz":480,"elapsed":540,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"4d62c089-860e-454d-dc92-44d592119d60"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              Tweet text    Label\n","1      RT @Papapishu: Man it would fucking rule if we...  abusive\n","2      It is time to draw close to Him &#128591;&#127...   normal\n","3      if you notice me start to act different or dis...   normal\n","4      Forget unfollowers, I believe in growing. 7 ne...   normal\n","5      RT @Vitiligoprince: Hate Being sexually Frustr...  abusive\n","...                                                  ...      ...\n","99991  RT @shangros: my fucking queen https://t.co/wa...  abusive\n","99992  #Osteporosis treated with #PEMF - rebuild bone...   normal\n","99993  @LGUSAMobile why does my phone screen keeps fl...   normal\n","99994  #bigdata vs. #reality ... but equally applies ...   normal\n","99995  you can do whatever you choose, if you first g...   normal\n","\n","[85966 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-7b6a4d4a-2331-4d2e-9ec2-c528a3f47146\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet text</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>RT @Papapishu: Man it would fucking rule if we...</td>\n","      <td>abusive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>It is time to draw close to Him &amp;#128591;&amp;#127...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>if you notice me start to act different or dis...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Forget unfollowers, I believe in growing. 7 ne...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RT @Vitiligoprince: Hate Being sexually Frustr...</td>\n","      <td>abusive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99991</th>\n","      <td>RT @shangros: my fucking queen https://t.co/wa...</td>\n","      <td>abusive</td>\n","    </tr>\n","    <tr>\n","      <th>99992</th>\n","      <td>#Osteporosis treated with #PEMF - rebuild bone...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>99993</th>\n","      <td>@LGUSAMobile why does my phone screen keeps fl...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>99994</th>\n","      <td>#bigdata vs. #reality ... but equally applies ...</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>99995</th>\n","      <td>you can do whatever you choose, if you first g...</td>\n","      <td>normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>85966 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b6a4d4a-2331-4d2e-9ec2-c528a3f47146')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7b6a4d4a-2331-4d2e-9ec2-c528a3f47146 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7b6a4d4a-2331-4d2e-9ec2-c528a3f47146');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9ac24efc-0331-4b6d-802e-f099586b7725\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ac24efc-0331-4b6d-802e-f099586b7725')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9ac24efc-0331-4b6d-802e-f099586b7725 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_457013bc-97bf-45ca-b3da-5b8ac8762825\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ah_tweets')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_457013bc-97bf-45ca-b3da-5b8ac8762825 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('ah_tweets');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["ah_tweets[\"@ count\"] = ah_tweets['Tweet text'].str.count('@')\n","#Remove @'s from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('@[\\S]*', '')\n","\n","ah_tweets['Hashtag count'] = ah_tweets['Tweet text'].str.count('#')\n","#Remove #'s from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('#[\\S]*','')\n","\n","ah_tweets['Retweet'] = ah_tweets['Tweet text'].str.contains('RT')\n","#Remove RT's from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('RT', '')\n","\n","ah_tweets['Url count'] = ah_tweets['Tweet text'].str.count('http[\\S]*')\n","#Remove URL's from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('http[\\S]*','')\n","\n","ah_tweets['Uppercase word count'] = ah_tweets['Tweet text'].apply(lambda x: sum(map(str.isupper, x.split())))\n","\n","ah_tweets['Num sentences'] = ah_tweets['Tweet text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n","\n","ah_tweets['temp'] = ah_tweets['Tweet text'].apply(lambda x: nltk.sent_tokenize(x))\n","ah_tweets['Avg sentence len'] = ah_tweets['temp'].apply(lambda x: pd.Series(x).apply(lambda y: len(nltk.word_tokenize(y))).mean())\n","ah_tweets.drop(ah_tweets.columns[[8]], axis=1, inplace=True)\n","\n","#Got rid of ah_tweets with no text information after data cleaning/processing\n","ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: x.isspace())\n","ah_tweets = ah_tweets[ah_tweets['Text empty'] != True]\n","ah_tweets.drop(ah_tweets.columns[[9]], axis=1, inplace=True)\n","ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: len(x) == 0)\n","ah_tweets = ah_tweets[ah_tweets['Text empty'] != True]\n","ah_tweets.drop(ah_tweets.columns[[9]], axis=1, inplace=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E50pAXzQbxqK","executionInfo":{"status":"ok","timestamp":1703448848297,"user_tz":480,"elapsed":105011,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"76a0b7c1-e08d-4c1f-87d1-65aa716976df"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-e4e02b169367>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('@[\\S]*', '')\n","<ipython-input-6-e4e02b169367>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('#[\\S]*','')\n","<ipython-input-6-e4e02b169367>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('http[\\S]*','')\n","<ipython-input-6-e4e02b169367>:22: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  ah_tweets['Avg sentence len'] = ah_tweets['temp'].apply(lambda x: pd.Series(x).apply(lambda y: len(nltk.word_tokenize(y))).mean())\n","<ipython-input-6-e4e02b169367>:28: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ah_tweets.drop(ah_tweets.columns[[9]], axis=1, inplace=True)\n","<ipython-input-6-e4e02b169367>:29: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: len(x) == 0)\n","<ipython-input-6-e4e02b169367>:31: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ah_tweets.drop(ah_tweets.columns[[9]], axis=1, inplace=True)\n"]}]},{"cell_type":"code","source":["sia = SentimentIntensityAnalyzer() #Sentiment is 'compound' value from -1 to 1\n","ah_tweets['Sentiment'] = ah_tweets['Tweet text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n","\n","#Create temperorary parts of speech column\n","from collections import Counter\n","ah_tweets['temp'] = ah_tweets['Tweet text'].apply(lambda x: nltk.pos_tag(x.split()))\n","ah_tweets['temp'] = ah_tweets['temp'].apply(lambda x: Counter(tag for word,tag in x))\n","\n","#Counting all adjectives\n","ah_tweets['Adjective count'] = ah_tweets['temp'].apply(lambda x: x['JJ'])\n","ah_tweets['Adjective count'] = ah_tweets['Adjective count'] + ah_tweets['temp'].apply(lambda x: x['JJR'])\n","ah_tweets['Adjective count'] = ah_tweets['Adjective count'] + ah_tweets['temp'].apply(lambda x: x['JJS'])\n","\n","#Counting all verbs\n","ah_tweets['Verb count'] = ah_tweets['temp'].apply(lambda x: x['VB'])\n","ah_tweets['Verb count'] = ah_tweets['Verb count'] + ah_tweets['temp'].apply(lambda x: x['VBP'])\n","ah_tweets['Verb count'] = ah_tweets['Verb count'] + ah_tweets['temp'].apply(lambda x: x['VBD'])\n","ah_tweets['Verb count'] = ah_tweets['Verb count'] + ah_tweets['temp'].apply(lambda x: x['VBG'])\n","ah_tweets['Verb count'] = ah_tweets['Verb count'] + ah_tweets['temp'].apply(lambda x: x['VBN'])\n","ah_tweets['Verb count'] = ah_tweets['Verb count'] + ah_tweets['temp'].apply(lambda x: x['VBZ'])\n","\n","#Counting all adverbs\n","ah_tweets['Adverb count'] = ah_tweets['temp'].apply(lambda x: x['RB'])\n","ah_tweets['Adverb count'] = ah_tweets['Adverb count'] + ah_tweets['temp'].apply(lambda x: x['RBR'])\n","ah_tweets['Adverb count'] = ah_tweets['Adverb count'] + ah_tweets['temp'].apply(lambda x: x['RBS'])\n","\n","#Remove temperoary parts of speech column\n","ah_tweets.drop(ah_tweets.columns[[10]], axis=1, inplace=True)\n","\n","#Remove punctuation from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('[^\\w\\s]', '')\n","#Remove numbers from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('\\d', '')\n","#Remove non-letters from tweet text\n","ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('[^a-zA-Z]', ' ')\n","\n","#Got rid of ah_tweets with no text information after data cleaning/processing\n","ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: x.isspace())\n","ah_tweets = ah_tweets[ah_tweets['Text empty'] != True]\n","ah_tweets.drop(ah_tweets.columns[[13]], axis=1, inplace=True)\n","ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: len(x) == 0)\n","ah_tweets = ah_tweets[ah_tweets['Text empty'] != True]\n","ah_tweets.drop(ah_tweets.columns[[13]], axis=1, inplace=True)\n","\n","ah_tweets['Profanity count'] = ah_tweets['Tweet text'].str.replace('\\W', ' ').str.split()\n","ah_tweets['Profanity count'] = ah_tweets['Profanity count'].apply(lambda x: sum(predict(x)))\n","\n","ah_tweets['Avg word len'] = ah_tweets['Tweet text'].apply(lambda x: sum(list(map(len,x.split()))) / len(x.split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MxGIymMmb5Se","executionInfo":{"status":"ok","timestamp":1703449264483,"user_tz":480,"elapsed":416195,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"560b470f-c0bf-4ffe-a7fa-ac8fe67f1fd3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-b4c6cbbd29cf>:31: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('[^\\w\\s]', '')\n","<ipython-input-7-b4c6cbbd29cf>:33: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('\\d', '')\n","<ipython-input-7-b4c6cbbd29cf>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Tweet text'] = ah_tweets['Tweet text'].str.replace('[^a-zA-Z]', ' ')\n","<ipython-input-7-b4c6cbbd29cf>:40: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ah_tweets.drop(ah_tweets.columns[[13]], axis=1, inplace=True)\n","<ipython-input-7-b4c6cbbd29cf>:41: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  ah_tweets['Text empty'] = ah_tweets['Tweet text'].apply(lambda x: len(x) == 0)\n","<ipython-input-7-b4c6cbbd29cf>:45: FutureWarning: The default value of regex will change from True to False in a future version.\n","  ah_tweets['Profanity count'] = ah_tweets['Tweet text'].str.replace('\\W', ' ').str.split()\n"]}]},{"cell_type":"code","source":["ah_tweets_copy = ah_tweets.copy()\n","ah_tweets"],"metadata":{"id":"1-OXig5UnRb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ah_tweets.drop(ah_tweets.columns[[0]], axis=1, inplace=True)\n","category = encode_text_index(ah_tweets, \"Label\")\n","#'abusive' ah_tweets label = 0\n","#'hateful' ah_tweets label = 1\n","#'normal' ah_tweets label = 2\n","encode_numeric_zscore(ah_tweets, \"@ count\")\n","encode_numeric_zscore(ah_tweets, \"Hashtag count\")\n","encode_text_dummy(ah_tweets, \"Retweet\")\n","encode_numeric_zscore(ah_tweets, \"Url count\")\n","encode_numeric_zscore(ah_tweets, \"Uppercase word count\")\n","encode_numeric_zscore(ah_tweets, \"Num sentences\")\n","encode_numeric_zscore(ah_tweets, \"Adjective count\")\n","encode_numeric_zscore(ah_tweets, \"Verb count\")\n","encode_numeric_zscore(ah_tweets, \"Adverb count\")\n","encode_numeric_zscore(ah_tweets, \"Profanity count\")\n","ah_tweets"],"metadata":{"id":"msgWV61-cA1Y","executionInfo":{"status":"ok","timestamp":1703449264855,"user_tz":480,"elapsed":378,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Create A/H Tweets Data Stream"],"metadata":{"id":"kZsCiE1Dch8r"}},{"cell_type":"code","source":["x_ah = ah_tweets.iloc[:,1:]\n","y_ah = ah_tweets['Label']\n","x_train_ah, x_test_ah, y_train_ah, y_test_ah = train_test_split(x_ah, y_ah, test_size=0.3, random_state=42)\n","\n","# The labels are set as the kafka message keys so as to store data\n","# in multiple-partitions. Thus, enabling efficient data retrieval\n","# using the consumer groups.\n","x_train_ah = list(filter(None, x_train_ah.to_csv(index=False).split(\"\\n\")[1:]))\n","y_train_ah = list(filter(None, y_train_ah.to_csv(index=False).split(\"\\n\")[1:]))\n","\n","x_test_ah = list(filter(None, x_test_ah.to_csv(index=False).split(\"\\n\")[1:]))\n","y_test_ah = list(filter(None, y_test_ah.to_csv(index=False).split(\"\\n\")[1:]))"],"metadata":{"id":"mSW36YZ0HFZC","executionInfo":{"status":"ok","timestamp":1703449626038,"user_tz":480,"elapsed":3065,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["write_to_kafka(\"ah-tweets-train\", zip(x_train_ah, y_train_ah))\n","write_to_kafka(\"ah-tweets-test\", zip(x_test_ah, y_test_ah))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gb8VwsDCyLt","executionInfo":{"status":"ok","timestamp":1703449645926,"user_tz":480,"elapsed":16069,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"3809b9fa-960d-4a95-fc1a-e6fec396a17c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote 60027 messages into topic: ah-tweets-train\n","Wrote 25727 messages into topic: ah-tweets-test\n"]}]},{"cell_type":"markdown","source":["Consume A/H Tweets From Data Stream"],"metadata":{"id":"QqjDzYsog2OL"}},{"cell_type":"code","source":["online_train_ah_tweets = tfio.experimental.streaming.KafkaBatchIODataset(\n","    topics=[\"ah-tweets-train\"],\n","    group_id=\"online-train-ah\",\n","    servers=\"127.0.0.1:9092\",\n","    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n","    configuration=[\n","        \"session.timeout.ms=7000\",\n","        \"max.poll.interval.ms=8000\",\n","        \"auto.offset.reset=earliest\"\n","    ],\n",")\n","\n","test_ah_tweets = tfio.experimental.streaming.KafkaGroupIODataset(\n","    topics=[\"ah-tweets-test\"],\n","    group_id=\"test-ah\",\n","    servers=\"127.0.0.1:9092\",\n","    stream_timeout=10000,\n","    configuration=[\n","        \"session.timeout.ms=7000\",\n","        \"max.poll.interval.ms=8000\",\n","        \"auto.offset.reset=earliest\"\n","    ],\n",")"],"metadata":{"id":"rC_Ug8n4DOm2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"test_ah_tweets = test_ah_tweets.map(decode_kafka_test_item)\n","test_ah_tweets = test_ah_tweets.batch(BATCH_SIZE)\"\"\"\n","\n","for mini_ds in online_train_ah_tweets:\n","  mini_ds = mini_ds.shuffle(buffer_size=32)\n","  mini_ds = mini_ds.map(decode_kafka_online_item)\n","  mini_ds = mini_ds.batch(32)\n","  if len(mini_ds) > 0:\n","    print(mini_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"097X7if2E6UV","executionInfo":{"status":"ok","timestamp":1703297506967,"user_tz":480,"elapsed":11119,"user":{"displayName":"Raul Arambula","userId":"15420229935369107316"}},"outputId":"f133c4ce-faaa-4430-f20b-5ef18eaf88bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n","<_BatchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"]}]},{"cell_type":"markdown","source":["Aggressive Tweets"],"metadata":{"id":"ieZQk6JgnjkV"}},{"cell_type":"code","source":["#combine abusive and hateful tweets into aggressive for later comparison with other work\n","aggressive_tweets = ah_tweets_copy\n","aggressive_tweets = aggressive_tweets.replace(\"abusive\",\"aggressive\")\n","aggressive_tweets = aggressive_tweets.replace(\"hateful\",\"aggressive\")\n","aggressive_tweets"],"metadata":{"id":"FiXb07l1FycC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check to make sure only aggressive and normal tweet labels exist\n","pd.unique(aggressive_tweets['Label'])"],"metadata":{"id":"oygs6T_SnxQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aggressive_tweets.drop(aggressive_tweets.columns[[0]], axis=1, inplace=True)\n","category = encode_text_index(aggressive_tweets, \"Label\")\n","#'aggressive' tweets label = 0\n","#'normal' tweets label = 1\n","encode_numeric_zscore(aggressive_tweets, \"@ count\")\n","encode_numeric_zscore(aggressive_tweets, \"Hashtag count\")\n","encode_text_dummy(aggressive_tweets, \"Retweet\")\n","encode_numeric_zscore(aggressive_tweets, \"Url count\")\n","encode_numeric_zscore(aggressive_tweets, \"Uppercase word count\")\n","encode_numeric_zscore(aggressive_tweets, \"Num sentences\")\n","encode_numeric_zscore(aggressive_tweets, \"Adjective count\")\n","encode_numeric_zscore(aggressive_tweets, \"Verb count\")\n","encode_numeric_zscore(aggressive_tweets, \"Adverb count\")\n","encode_numeric_zscore(aggressive_tweets, \"Profanity count\")\n","aggressive_tweets"],"metadata":{"id":"PYK8Iffun3B8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create Aggressive Tweets Data Stream"],"metadata":{"id":"pavax7q3oHGW"}},{"cell_type":"code","source":["x_agg = aggressive_tweets.iloc[:,1:]\n","y_agg = aggressive_tweets['Label']\n","x_train_agg, x_test_agg, y_train_agg, y_test_agg = train_test_split(x_agg, y_agg, test_size=0.3, random_state=42)\n","\n","# The labels are set as the kafka message keys so as to store data\n","# in multiple-partitions. Thus, enabling efficient data retrieval\n","# using the consumer groups.\n","x_train_agg = list(filter(None, x_train_agg.to_csv(index=False).split(\"\\n\")[1:]))\n","y_train_agg = list(filter(None, y_train_agg.to_csv(index=False).split(\"\\n\")[1:]))\n","\n","x_test_agg = list(filter(None, x_test_agg.to_csv(index=False).split(\"\\n\")[1:]))\n","y_test_agg = list(filter(None, y_test_agg.to_csv(index=False).split(\"\\n\")[1:]))"],"metadata":{"id":"_zzqhOU4oIzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["write_to_kafka(\"agg-tweets-train\", zip(x_train_agg, y_train_agg))\n","write_to_kafka(\"agg-tweets-test\", zip(x_test_agg, y_test_agg))"],"metadata":{"id":"88is78kFoezM"},"execution_count":null,"outputs":[]}]}